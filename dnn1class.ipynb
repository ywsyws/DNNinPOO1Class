{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnnOneClass:\n",
    "\n",
    "    \n",
    "    ''' create datasets '''\n",
    "\n",
    "    def create_dataset(self, col_num):\n",
    "        # create entry dataset X (train or test)\n",
    "        self.X = np.random.randint(2, size=(2,col_num))\n",
    "\n",
    "        # create label dataset Y (train or test)\n",
    "        self.Y = np.sum(X, axis=0, keepdims=True)\n",
    "        self.Y[self.Y!=1] = 0\n",
    "\n",
    "        # create noises in the entry dataset X by adding (-0.6, 0.6) to the data\n",
    "        self.X_noise = np.random.randn(2,col_num)\n",
    "        self.X_noise = self.X + (self.X_noise / 20)\n",
    "\n",
    "        return self.X_noise, self.Y\n",
    "\n",
    "\n",
    "    ''' initialize parameters -- W, b '''\n",
    "    def initialize_parameters(self, layer_dims):\n",
    "\n",
    "        self.parameters = {}\n",
    "        self.L = len(layer_dims)\n",
    "\n",
    "        # for the first L-1 layers, we use a heristic to initialize weight that is customized to the relu function\n",
    "        for i in range(1, self.L-1):\n",
    "            self.parameters[f'W{i}'] = np.random.randn(layer_dims[i], layer_dims[i-1]) * self.relu(None, heuristic=layer_dims[i-1])\n",
    "            self.parameters[f'b{i}'] = np.zeros((layer_dims[i], 1))\n",
    "\n",
    "        # for the last layer (L), we use a heristic to initialize weight that is customized to the sigmoid function\n",
    "        self.parameters[f'W{L-1}'] = np.random.randn(layer_dims[L-1], layer_dims[L-2]) * self.sigmoid(None, heuristic=layer_dims[L-2])\n",
    "        self.parameters[f'b{L-1}'] = np.zeros((layer_dims[L-1], 1))\n",
    "\n",
    "        return self.parameters\n",
    "\n",
    "\n",
    "    ''' define activation function (sigmoid) and its derivative '''\n",
    "    def sigmoid(self, F, derivative=False, heuristic=False):\n",
    "\n",
    "        # calculate the derivative of sigmoid\n",
    "        if derivative:\n",
    "            return F * (1 - F) # F = A\n",
    "\n",
    "        # calculate the heuristic to initialize weight that is customized to the sigmoid function \n",
    "        if heuristic:\n",
    "            return np.sqrt(1 / heuristic)\n",
    "\n",
    "        # calucate the sigmoid function\n",
    "        else:\n",
    "            return 1 / (1 + np.exp(-F)) # F = Z\n",
    "\n",
    "    def relu(self, F, derivative=False, heuristic=False):\n",
    "\n",
    "        # calculate the derivative of relu\n",
    "        if derivative:\n",
    "            return 1 * (F > 0) # F = Z\n",
    "\n",
    "        # calculate the heuristic to initialize weight that is customized to the relu function\n",
    "        elif heuristic:\n",
    "            return np.sqrt(2 / heuristic)\n",
    "\n",
    "        # calucate the relu function\n",
    "        else:\n",
    "            return F * (F > 0) # F = Z\n",
    "\n",
    "\n",
    "    ''' 1. forward propagation function - calculate pre-activation fn (Z) & activation fn (A) ''' \n",
    "\n",
    "    def forward_pass(self, X, parameters, layer_nums):\n",
    "\n",
    "        self.cache = {}\n",
    "        self.cache['A0'] = X\n",
    "        self.L = len(layer_nums)\n",
    "\n",
    "        for i in range(1, self.L-1):\n",
    "\n",
    "            # for the first L-1 layers, use relu as an activation function\n",
    "            self.cache[f'Z{i}'] = np.dot(parameters[f'W{i}'], self.cache[f'A{i-1}']) + parameters[f'b{i}']\n",
    "            self.cache[f'A{i}'] = self.relu(self.cache[f'Z{i}'])\n",
    "\n",
    "        # for the last layer L, use sigmoid as an activation function\n",
    "        self.cache[f'Z{L-1}'] = np.dot(parameters[f'W{L-1}'], self.cache[f'A{L-2}']) + parameters[f'b{L-1}']\n",
    "        self.cache[f'A{L-1}'] = self.sigmoid(self.cache[f'Z{L-1}'])\n",
    "\n",
    "        return self.cache\n",
    "\n",
    "\n",
    "    ''' 2. calculate cost '''\n",
    "    def cost(self, A, Y):\n",
    "\n",
    "        self.m = Y.shape[1]\n",
    "        self.J = - np.sum (Y * np.log(A) + (1 - Y) * np.log(1 - A)) / self.m\n",
    "\n",
    "        return self.J\n",
    "\n",
    "\n",
    "    ''' 3. backward propagation fonction - calculate dW & db from dA & dZ'''\n",
    "\n",
    "    # backward non_linear function to calculate dA & dZ\n",
    "    def backward_pass(self, cache, parameters, Y, layer_dims):\n",
    "\n",
    "        self.grads = {}\n",
    "        self.m = Y.shape[1]\n",
    "        self.L = len(layer_dims)\n",
    "\n",
    "        # for last layer, use the derivative of sigmoid, which is simply (A-Y). So no need to call the sigmoid function\n",
    "        self.dZ = self.cache[f'A{self.L-1}'] - Y\n",
    "        self.grads[f'dW{L-1}'] = np.dot(self.dZ, self.cache[f'A{L-2}'].T) / self.m\n",
    "        self.grads[f'db{L-1}'] = np.sum(self.dZ, axis = 1, keepdims = True) / self.m \n",
    "\n",
    "        # for L-1 precendent layers, use the derivative of relu\n",
    "        for i in range(self.L-2, 0, -1):\n",
    "            self.dA = np.dot(self.parameters[f'W{i+1}'].T, self.dZ)\n",
    "            self.dZ = self.dA * self.relu(self.cache[f'Z{i}'], derivative=True)\n",
    "            self.grads[f'dW{i}'] = np.dot(self.dZ, self.cache[f'A{i-1}'].T) / self.m\n",
    "            self.grads[f'db{i}'] = np.sum(self.dZ, axis = 1, keepdims = True) / self.m\n",
    "\n",
    "        return self.grads\n",
    "\n",
    "\n",
    "    ''' 4. update parameters - W & b'''\n",
    "\n",
    "    # update parameters W & b using the gradients that were calculated from the backward pass\n",
    "    def update_parameters(self, parameters, grads, learning_rate, layer_dims):\n",
    "\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.parameters[f'W{i}'] -= learning_rate * self.grads[f'dW{i}']\n",
    "            self.parameters[f'b{i}'] -= learning_rate * self.grads[f'db{i}']\n",
    "\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DnnOneClass()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
