{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnnOneClass:\n",
    "\n",
    "    \n",
    "    ''' create datasets '''\n",
    "\n",
    "    def create_dataset(self, col_num):\n",
    "        # create entry dataset X (train or test)\n",
    "        self.X = np.random.randint(2, size=(2,col_num))\n",
    "\n",
    "        # create label dataset Y (train or test)\n",
    "        self.Y = np.sum(self.X, axis=0, keepdims=True)\n",
    "        self.Y[self.Y!=1] = 0\n",
    "\n",
    "        # create noises in the entry dataset X by adding (-0.6, 0.6) to the data\n",
    "        self.X_noise = np.random.randn(2,col_num)\n",
    "        self.X_noise = self.X + (self.X_noise / 20)\n",
    "\n",
    "        return self.X_noise, self.Y\n",
    "\n",
    "\n",
    "    ''' initialize parameters -- W, b '''\n",
    "    def initialize_parameters(self, layer_dims):\n",
    "\n",
    "        self.parameters = {}\n",
    "        self.L = len(layer_dims)\n",
    "\n",
    "        # for the first L-1 layers, we use a heristic to initialize weight that is customized to the relu function\n",
    "        for i in range(1, self.L-1):\n",
    "            self.parameters[f'W{i}'] = np.random.randn(layer_dims[i], layer_dims[i-1]) * self.relu(None, heuristic=layer_dims[i-1])\n",
    "            self.parameters[f'b{i}'] = np.zeros((layer_dims[i], 1))\n",
    "\n",
    "        # for the last layer (L), we use a heristic to initialize weight that is customized to the sigmoid function\n",
    "        self.parameters[f'W{self.L-1}'] = np.random.randn(layer_dims[self.L-1], layer_dims[self.L-2]) * self.sigmoid(None, heuristic=layer_dims[self.L-2])\n",
    "        self.parameters[f'b{self.L-1}'] = np.zeros((layer_dims[self.L-1], 1))\n",
    "\n",
    "        return self.parameters\n",
    "\n",
    "\n",
    "    ''' define activation function (sigmoid) and its derivative '''\n",
    "    def sigmoid(self, F, derivative=False, heuristic=False):\n",
    "\n",
    "        # calculate the derivative of sigmoid\n",
    "        if derivative:\n",
    "            return F * (1 - F) # F = A\n",
    "\n",
    "        # calculate the heuristic to initialize weight that is customized to the sigmoid function \n",
    "        if heuristic:\n",
    "            return np.sqrt(1 / heuristic)\n",
    "\n",
    "        # calucate the sigmoid function\n",
    "        else:\n",
    "            return 1 / (1 + np.exp(-F)) # F = Z\n",
    "\n",
    "    def relu(self, F, derivative=False, heuristic=False):\n",
    "\n",
    "        # calculate the derivative of relu\n",
    "        if derivative:\n",
    "            return 1 * (F > 0) # F = Z\n",
    "\n",
    "        # calculate the heuristic to initialize weight that is customized to the relu function\n",
    "        elif heuristic:\n",
    "            return np.sqrt(2 / heuristic)\n",
    "\n",
    "        # calucate the relu function\n",
    "        else:\n",
    "            return F * (F > 0) # F = Z\n",
    "\n",
    "\n",
    "    ''' 1. forward propagation function - calculate pre-activation fn (Z) & activation fn (A) ''' \n",
    "\n",
    "    def forward_pass(self, X, parameters, layer_nums):\n",
    "\n",
    "        self.cache = {}\n",
    "        self.cache['A0'] = X\n",
    "        self.L = len(layer_nums)\n",
    "\n",
    "        for i in range(1, self.L-1):\n",
    "\n",
    "            # for the first L-1 layers, use relu as an activation function\n",
    "            self.cache[f'Z{i}'] = np.dot(parameters[f'W{i}'], self.cache[f'A{i-1}']) + parameters[f'b{i}']\n",
    "            self.cache[f'A{i}'] = self.relu(self.cache[f'Z{i}'])\n",
    "\n",
    "        # for the last layer L, use sigmoid as an activation function\n",
    "        self.cache[f'Z{self.L-1}'] = np.dot(parameters[f'W{self.L-1}'], self.cache[f'A{self.L-2}']) + parameters[f'b{self.L-1}']\n",
    "        self.cache[f'A{self.L-1}'] = self.sigmoid(self.cache[f'Z{self.L-1}'])\n",
    "\n",
    "        return self.cache\n",
    "\n",
    "\n",
    "    ''' 2. calculate cost '''\n",
    "    def cost(self, A, Y):\n",
    "\n",
    "        self.m = Y.shape[1]\n",
    "        self.J = - np.sum (Y * np.log(A) + (1 - Y) * np.log(1 - A)) / self.m\n",
    "\n",
    "        return self.J\n",
    "\n",
    "\n",
    "    ''' 3. backward propagation fonction - calculate dW & db from dA & dZ'''\n",
    "\n",
    "    # backward non_linear function to calculate dA & dZ\n",
    "    def backward_pass(self, cache, parameters, Y, layer_dims):\n",
    "\n",
    "        self.grads = {}\n",
    "        self.m = Y.shape[1]\n",
    "        self.L = len(layer_dims)\n",
    "\n",
    "        # for last layer, use the derivative of sigmoid, which is simply (A-Y). So no need to call the sigmoid function\n",
    "        self.dZ = self.cache[f'A{self.L-1}'] - Y\n",
    "        self.grads[f'dW{self.L-1}'] = np.dot(self.dZ, self.cache[f'A{self.L-2}'].T) / self.m\n",
    "        self.grads[f'db{self.L-1}'] = np.sum(self.dZ, axis = 1, keepdims = True) / self.m \n",
    "\n",
    "        # for L-1 precendent layers, use the derivative of relu\n",
    "        for i in range(self.L-2, 0, -1):\n",
    "            self.dA = np.dot(self.parameters[f'W{i+1}'].T, self.dZ)\n",
    "            self.dZ = self.dA * self.relu(self.cache[f'Z{i}'], derivative=True)\n",
    "            self.grads[f'dW{i}'] = np.dot(self.dZ, self.cache[f'A{i-1}'].T) / self.m\n",
    "            self.grads[f'db{i}'] = np.sum(self.dZ, axis = 1, keepdims = True) / self.m\n",
    "\n",
    "        return self.grads\n",
    "\n",
    "\n",
    "    ''' 4. update parameters - W & b'''\n",
    "\n",
    "    # update parameters W & b using the gradients that were calculated from the backward pass\n",
    "    def update_parameters(self, parameters, grads, learning_rate, layer_dims):\n",
    "\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.parameters[f'W{i}'] -= learning_rate * self.grads[f'dW{i}']\n",
    "            self.parameters[f'b{i}'] -= learning_rate * self.grads[f'db{i}']\n",
    "\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(col_num, layer_dims, learning_rate, iteration):\n",
    "    ''' training the model '''\n",
    "\n",
    "    X, Y = dnn.create_dataset(col_num)\n",
    "    parameters = dnn.initialize_parameters(layer_dims)\n",
    "\n",
    "    for i in range(iteration):\n",
    "\n",
    "        # 1. forward propagation\n",
    "        cache = dnn.forward_pass(X, parameters, layer_dims)\n",
    "        # 2. cost function\n",
    "        J = dnn.cost(cache[f'A{len(layer_dims)-1}'], Y)\n",
    "        # 3. backward propagation\n",
    "        grads = dnn.backward_pass(cache, parameters, Y, layer_dims)\n",
    "        # 4. update parameters\n",
    "        parameters = dnn.update_parameters(parameters, grads, learning_rate, layer_dims)\n",
    "\n",
    "        if i % 5000 == 0:\n",
    "            print(f'cost{i}: {J}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost0: 0.7499728943129313\n",
      "cost5000: 0.0014123205031113385\n",
      "cost10000: 0.0006027738236117275\n",
      "cost15000: 0.00037222070797348386\n",
      "cost20000: 0.00026581872142963816\n",
      "cost25000: 0.000205252388581657\n",
      "cost30000: 0.00016640763806800512\n",
      "cost35000: 0.00013949024908722994\n",
      "cost40000: 0.00011979660613582231\n",
      "cost45000: 0.00010479548114371751\n"
     ]
    }
   ],
   "source": [
    "dnn = DnnOneClass()\n",
    "\n",
    "col_num = 10000\n",
    "layer_dims = 2, 5, 1\n",
    "learning_rate = 0.1\n",
    "iteration = 50000\n",
    "\n",
    "train(col_num, layer_dims, learning_rate, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
